\documentclass[a4paper, 8pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{rotating}

\usepackage[a4paper,
            left=0.6in,
            right=0.6in,
            top=1in,
            bottom=1in]{geometry}

\usepackage{hyperref}
\usepackage{lipsum}

\usepackage{multicol}
\setlength{\columnsep}{0.3cm}

% Title Setup
\title{Application of High Dimensional Techniques to House Price Prediction}

\vspace{5pt}

\author{
  Pala, Alessandro\\
  \texttt{alessandro.pala@studenti.unipd.it}
  \and
  Tremaggi, Domenico\\
  \texttt{domenico.tremaggi@studenti.unipd.it}
}
\date{}
\begin{document}

\maketitle
\noindent
\begin{abstract}
This experiment explores two different sets of grouped variables through a Group Lasso regularized regression model and compares them to a basic regression model, a more complex regression model and a deep Multi-Layer Perceptron in the context of predicting house prices. The results... TO COMPLETE
\end{abstract}
\vspace{5pt}

\begin{multicols}{2}

\section{Introduction}
In order to approach the dataset, we first think about which models and variables can predict real estate prices in the real world, with the constraint of the set of variables given by the dataset. Since we handle a price prediction regression, a fair comparison of linear and non-linear basic models - be them regularized or not - would be multivariate. Hence, we first carry a synthetic application of the following models: multivariate linear model and multivariate Poisson regression with an $\alpha = 0.1$ Elastic Net.

We then train a deep Multi-Layer Perceptron for performance comparison and we regularize it with a simple Lasso.
Finally, we utilise a Group Lasso regression on two sets of groups of variables that we think may be good predictors of house prices in the real world. The first set of groups uses variables that are usually clustered together by real estate professionals when they do house price estimation. The second set of groups uses clusters of highly statistically correlated (Pearson's $r$) variables.

All selection for regularized models is carried through with cross-validation.

\section{Dataset}
The dataset contains real estate property listings, each described by a variety of attributes. Temporal attributes are: the listing date, the year the property was built, and the year of its last renovation. Physical characteristics are: the number of bedrooms and bathrooms, living area size, total lot size, number of floors, basement area, and living area above ground. Scenic qualities are represented by a waterfront view indicator, a view quality rating, an overall condition rating, and an overall grade rating. Locational data instead include zip code,  latitude, and longitude. There is also some additional spatial context given by the average living area and lot sizes of the 15 nearest properties.

\subsection{Pre-processing}
In order to pre-process the data, we first convert the date column into a proper date object by extracting the first 8 characters (in "YYYYMMDD" format). We then replace 0 values in the "yr{\_}renovated" column with NA, treating them as missing data. Duplicate rows are removed based on the id column.
We add new variables in the dataset: total{\_}sqft calculates total area by summing living and basement space, bath{\_}per{\_}bed computes the bathroom-to-bedroom ratio (defaulting to 0 if bedrooms are 0), and total{\_}rooms sums bedrooms and bathrooms. sqft{\_}diff{\_}15 measures the difference between the property’s living area and that of its 15 nearest neighbors, while age{\_}since{\_}reno calculates the property’s age since construction, defaulting to 0 if the year is invalid.
The first, second, fifteenth and sixteenth columns are dropped since they are analytically irrelevant (date, id, with yr{\_}built and yr{\_}renovated being substituted by "age"). We then scale the features of the data frame.
The price and the non-structural features are extracted as columns 2 to 18 and scaled in the new matrix X, which will be used for the Group Lasso. The price, or target, is stored in a separate vector Y.

\section{Base Models}
\subsection{Multivariate linear model}
Multivariate Linear Regression is an extension of linear regression where the goal is to model the relationship between multiple independent variables (also called predictors or features) and a dependent variable (also called the target or response variable). It is used when you have more than one predictor variable that you believe influences the dependent variable. \newpage
\subsubsection*{Mathematical equation}
\begin{equation}
Y=\beta_0+\beta_1X_1+\beta_2X_2+\dots+\beta_kX_k+\epsilon
\end{equation}

where:$Y$ is the dependent variable, ($X_1, X_2,\dots,X_k$) are the independent variables, $\beta_0$ is the intercept, ($\beta_1,\beta_2, \dots, \beta_k$) are the coefficients and $\epsilon$ is the error term.\\

In our case of interest we use all the the available variables to estimate the house prices.
The multivariate linear regression with all original features proved a very sparse interpretation from a p-value standpoint, with only one coefficient - the number of bedrooms being a solid price predictor. We try with some more complex models that also use Elastic Net to check whether the sparsity is justified.


\subsection{Multivariate GLM with Elastic Net}
\subsubsection{The Poisson Distribution}
Poisson regression is based on the Poisson distribution, which describes the probability of a given number of events occurring in a fixed interval of time or space, given the average rate of occurrence.

The probability mass function of a Poisson-distributed random variable $Y$ is:
\begin{equation}
P(Y=y)= \frac{\lambda^ye^{-\lambda}}{y!}
\end{equation}
Where: $y$ is the count of events (e.g., number of accidents, number of emails received) and $\lambda$ is the rate of occurrence, which is the expected number of events in the fixed interval.
\subsubsection{The Poisson Regression Model}
Poisson regression assumes that the logarithm of the expected count (rate) is a linear function of the independent variables.\\
The model is expressed as:
\begin{equation}
\log(\lambda_i)=\beta_0+\beta_1X_1+\beta_2X_2+\dots+\beta_kX_k
\end{equation}
where: ($X_1, X_2,\dots,X_k$) are the indipendent variables, $\beta_0$ is the intercept, ($\beta_1,\beta_2, \dots, \beta_k$) are the regression coefficients
\subsubsection{Elastic Net}
In regular linear regression or GLMs, the model can overfit the data if there are many predictors, especially when some predictors are irrelevant or highly correlated, but it can also use some sparsity enforcement, so we applied an Elastic Net regression. This kind of regularization combines $L_1$ and $L_2$ regularizations and applies penalty to coefficients as to: 
\begin{itemize}
\item Lasso ($L_1$1): Shrink some coefficients to zero, performing feature selection.
\item Ridge ($L_2$): Shrink the coefficients (but not set them to zero) helping with multicollinearity, which our dataset suffers from.
\end{itemize}
Mathematically, the Elastic Net penalty is given by:
\begin{equation}
\text{Penalty}= \gamma \left[\alpha \sum_{j=1}^k|\beta_j| + \frac{1}{2}(1-\alpha)\sum_{j=1}^k\beta_{j}^{2} \right]
\end{equation} 
where $\gamma$ controls the strength of the regularization (larger $\gamma$ means more regularization), $\alpha$ controls the balance between Lasso and Ridge. If $\alpha=1$, the penalty is purely Lasso; if $\alpha=0$, the penalty is purely Ridge, meaning our $\alpha = 0.1$ choice mostly applies an $L_2$ penalty. We chose this hyperparameter after some experiments that suggested that even a very small Lasso penalty enforced sparsity efficiently.
\subsubsection{Poisson Regression with Elastic Net}
When you combine Poisson regression with Elastic Net regularization the model becomes:
\begin{equation}
\begin{split}
\log(\lambda_i)&=\beta_0+\beta_1X_1+\beta_2X_2+\dots+\beta_kX_k+ \\
&+\gamma \left[\alpha \sum_{j=1}^k|\beta_j| + \frac{1}{2}(1-\alpha)\sum_{j=1}^k\beta_{j}^{2} \right]
\end{split}
\end{equation}
This means that the objective is to minimize the following loss function:
\begin{equation*}
\text{Loss}=\sum_{i=1}^n(-Y_i \log(\lambda_i)+ \lambda_i)+ \gamma \left[\alpha \sum_{j=1}^k|\beta_j| + \frac{1}{2}(1-\alpha)\sum_{j=1}^k\beta_{j}^{2} \right]
\end{equation*}

\subsubsection{Experiments}
We hence proceed with fitting our Poisson regression to the data. 


\section{Neural Network}

\section{Group Lasso}
\subsection{The Group Lasso}
The Group Lasso is an extension of the classical Lasso method applied for regularization and variable selection designed to work with grouped variables, and such a situation is helpful when given variables naturally fall into groups, and we want to choose or discard entire groups of variables simultaneously rather than choosing or dropping individual variables.

The main mathematical goal of the Group Lasso is to do regression by penalizing the sum of the norms of the coefficients in each group. That will induce sparsity at the level of groups, that is, all coefficients in a group will either be shrunk towards zero, or not. Therefore, entire groups of variables are either selected or not.

\subsubsection*{Mathematical Formulation}
For a given linear regression problem, the model can be represented as:
\begin{equation}
y=X\beta + \epsilon
\end{equation}
where: $y$ is the $n \times 1$ vector of observed responses, $X$ is the $n \times p$ matrix of predictors, $\beta$ is the $p \times 1$ vector of regression coefficient and $\epsilon$ is the error term, assumed to be normally distributed.\\

In ordinary linear regression, we try to minimize the residual squared error:
\begin{equation}
\min_{\beta}\left (\frac{1}{2n}\lVert y-X\beta\rVert^{2}_{2} \right)
\end{equation}

In the case of Group Lasso, we add a regularization term to the objective function that penalizes the coefficients in groups. If the variables are divided into $G$ groups, and group $g$ contains $p_{g}$ variables, the Group Lasso penalty is given by:
\begin{equation}
\lambda \sum_{g=1}^{G}\lVert \beta_{g} \rVert_{2}
\end{equation}

The idea would be: where: $\beta_g$ represents the coefficient corresponding to the $g$-th group of variables, $\lVert \beta_g \rVert_2$ is the euclidean norm of the coefficient vector, $\lambda$ is a regularization parameter that regulates its strength.

Thus, the optimization problem is:
\begin{equation}
\min_{\beta}\left (\frac{1}{2n}\lVert y-X\beta\rVert^{2}_{2} + \lambda \sum_{g=1}^{G}\lVert \beta_{g} \rVert_{2}\right)
\end{equation}

\subsection{Experiments}


\section{Conclusion}
\noindent

\end{multicols}


\pagebreak
\appendix

\section{Dataset}



\section{Visualization}

\section{Regressions}

\begin{table}[ht]
\centering
\begin{tabular}{|l|r|r|r|r|}
\hline
\textbf{Variable} & \textbf{Estimate} & \textbf{Std. Error} & \textbf{t value} & \textbf{p value} \\
\hline
(Intercept)       & 5.405e+05   & 4.201e-11  & 1.287e+16  & < 2e-16 ***  \\
bedrooms          & 3.677e+05   & 7.398e-11  & 4.970e+15  & < 2e-16 ***  \\
bathrooms         & 7.485e-11   & 5.423e-11  & 1.380e+00  & 0.168        \\
sqft$\_$living       & 3.946e-11   & 7.260e-11  & 5.440e-01  & 0.587        \\
sqft$\_$lot          & -1.557e-10  & 1.267e-10  & -1.229e+00 & 0.219        \\
floors            & 7.671e-12   & 6.084e-11  & 1.260e-01  & 0.900        \\
waterfront        & -1.241e-11  & 5.815e-11  & -2.130e-01 & 0.831        \\
view              & -6.033e-12  & 4.719e-11  & -1.280e-01 & 0.898        \\
condition         & 1.576e-11   & 5.097e-11  & 3.090e-01  & 0.757        \\
grade             & 3.389e-11   & 4.487e-11  & 7.550e-01  & 0.450        \\
sqft$\_$above        & 7.269e-11   & 7.782e-11  & 9.340e-01  & 0.350        \\
sqft$\_$basement     & -4.531e-11  & 1.110e-10  & -4.080e-01 & 0.683        \\
zipcode           & NA          & NA         & NA         & NA           \\
lat               & -6.270e-11  & 5.391e-11  & -1.163e+00 & 0.245        \\
long              & -2.108e-11  & 4.867e-11  & -4.330e-01 & 0.665        \\
sqft$\_$living15     & -1.568e-13  & 5.590e-11  & -3.000e-03 & 0.998        \\
sqft$\_$lot15        & 5.429e-11   & 7.241e-11  & 7.500e-01  & 0.453        \\
age               & 5.543e-12   & 6.138e-11  & 9.000e-02  & 0.928        \\
\hline
\end{tabular}
\caption{Multivariate Linear Regression Data Summary}
\end{table}


\begin{table}[ht]
\centering
\begin{tabular}{|l | r|}
\hline
\textbf{Variable} & \textbf{Coefficient} \\
\hline
Intercept     & 540529.7 \\
Bedrooms      & 356970.6 \\
Bathrooms     & . \\
Sqft\_Living   & . \\
Sqft\_Lot      & . \\
Floors        & . \\
Waterfront    & . \\
View          & . \\
Condition     & . \\
Grade         & . \\
Sqft\_Above    & . \\
Sqft\_Basement & . \\
Zipcode       & . \\
Lat           & . \\
Long          & . \\
Sqft\_Living15 & . \\
Sqft\_Lot15    & . \\
Age           & . \\
\hline
\end{tabular}
\caption{Coefficients from the Poisson model.}
\label{tab:mglm-coefs}
\end{table}


% Bibliography Section (if needed)
% \bibliographystyle{plain}
% \bibliography{references}

\end{document}
